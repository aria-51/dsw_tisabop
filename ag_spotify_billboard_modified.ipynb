{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: dask[complete] in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (2023.4.1)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[complete]) (2023.5.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[complete]) (6.3.0)\n",
      "Requirement already satisfied: click>=8.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[complete]) (8.1.3)\n",
      "Requirement already satisfied: cloudpickle>=1.5.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[complete]) (2.2.1)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[complete]) (0.12.0)\n",
      "Requirement already satisfied: partd>=1.2.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[complete]) (1.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[complete]) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[complete]) (6.0)\n",
      "Requirement already satisfied: lz4>=4.3.2 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[complete]) (4.3.2)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[complete]) (12.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.13.0->dask[complete]) (3.15.0)\n",
      "Requirement already satisfied: locket in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from partd>=1.2.0->dask[complete]) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from pyarrow>=7.0->dask[complete]) (1.24.3)\n",
      "Requirement already satisfied: jinja2>=2.10.3 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[complete]) (3.1.2)\n",
      "Requirement already satisfied: bokeh>=2.4.2 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[complete]) (3.1.1)\n",
      "Requirement already satisfied: pandas>=1.3 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[complete]) (2.0.1)\n",
      "Requirement already satisfied: distributed==2023.4.1 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[complete]) (2023.4.1)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.5 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed==2023.4.1->dask[complete]) (2.4.0)\n",
      "Requirement already satisfied: zict>=2.2.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed==2023.4.1->dask[complete]) (3.0.0)\n",
      "Requirement already satisfied: msgpack>=1.0.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed==2023.4.1->dask[complete]) (1.0.5)\n",
      "Requirement already satisfied: psutil>=5.7.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed==2023.4.1->dask[complete]) (5.9.4)\n",
      "Requirement already satisfied: tornado>=6.0.3 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed==2023.4.1->dask[complete]) (6.2)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed==2023.4.1->dask[complete]) (1.7.0)\n",
      "Requirement already satisfied: urllib3>=1.24.3 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed==2023.4.1->dask[complete]) (2.0.2)\n",
      "Requirement already satisfied: pillow>=7.1.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from bokeh>=2.4.2->dask[complete]) (9.5.0)\n",
      "Requirement already satisfied: contourpy>=1 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from bokeh>=2.4.2->dask[complete]) (1.0.7)\n",
      "Requirement already satisfied: xyzservices>=2021.09.1 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from bokeh>=2.4.2->dask[complete]) (2023.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from jinja2>=2.10.3->dask[complete]) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from pandas>=1.3->dask[complete]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from pandas>=1.3->dask[complete]) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from pandas>=1.3->dask[complete]) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from python-dateutil>=2.8.2->pandas>=1.3->dask[complete]) (1.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install \"dask[complete]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: dask-ml in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (2023.3.24)\n",
      "Requirement already satisfied: pandas>=0.24.2 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask-ml) (2.0.1)\n",
      "Requirement already satisfied: multipledispatch>=0.4.9 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask-ml) (0.6.0)\n",
      "Requirement already satisfied: dask[array,dataframe]>=2.4.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask-ml) (2023.4.1)\n",
      "Requirement already satisfied: scipy in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask-ml) (1.10.1)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask-ml) (1.24.3)\n",
      "Requirement already satisfied: packaging in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask-ml) (23.0)\n",
      "Requirement already satisfied: distributed>=2.4.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask-ml) (2023.4.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask-ml) (0.57.0)\n",
      "Requirement already satisfied: scikit-learn>=1.2.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask-ml) (1.2.2)\n",
      "Requirement already satisfied: dask-glm>=0.2.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask-ml) (0.2.0)\n",
      "Requirement already satisfied: cloudpickle>=0.2.2 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask-glm>=0.2.0->dask-ml) (2.2.1)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (6.0)\n",
      "Requirement already satisfied: click>=8.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (8.1.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (6.3.0)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (0.12.0)\n",
      "Requirement already satisfied: partd>=1.2.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (1.4.0)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (2023.5.0)\n",
      "Requirement already satisfied: jinja2>=2.10.3 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed>=2.4.0->dask-ml) (3.1.2)\n",
      "Requirement already satisfied: psutil>=5.7.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed>=2.4.0->dask-ml) (5.9.4)\n",
      "Requirement already satisfied: locket>=1.0.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed>=2.4.0->dask-ml) (1.0.0)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed>=2.4.0->dask-ml) (1.7.0)\n",
      "Requirement already satisfied: urllib3>=1.24.3 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed>=2.4.0->dask-ml) (2.0.2)\n",
      "Requirement already satisfied: zict>=2.2.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed>=2.4.0->dask-ml) (3.0.0)\n",
      "Requirement already satisfied: tornado>=6.0.3 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed>=2.4.0->dask-ml) (6.2)\n",
      "Requirement already satisfied: msgpack>=1.0.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed>=2.4.0->dask-ml) (1.0.5)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.5 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed>=2.4.0->dask-ml) (2.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.13.0->dask[array,dataframe]>=2.4.0->dask-ml) (3.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from jinja2>=2.10.3->distributed>=2.4.0->dask-ml) (2.1.2)\n",
      "Requirement already satisfied: six in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from multipledispatch>=0.4.9->dask-ml) (1.16.0)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from numba>=0.51.0->dask-ml) (0.40.0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from pandas>=0.24.2->dask-ml) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from pandas>=0.24.2->dask-ml) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from pandas>=0.24.2->dask-ml) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from scikit-learn>=1.2.0->dask-ml) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from scikit-learn>=1.2.0->dask-ml) (1.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install dask-ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.24.3'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning for Spotify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265669"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import relevant libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# import the dataset\n",
    "spotify_tracks = pd.read_csv('archive/tracks.csv')\n",
    "\n",
    "# drop duplicates with the same name and artist\n",
    "spotify_tracks = spotify_tracks.drop_duplicates(\n",
    "  subset = ['name', 'artists'],\n",
    "  keep = 'last').reset_index(drop = True)\n",
    "\n",
    "# convert release_time to appropriate time date format\n",
    "spotify_tracks['release_date']= pd.to_datetime(spotify_tracks['release_date'], format='mixed')\n",
    "\n",
    "# remove songs older than 1990\n",
    "spotify_tracks = spotify_tracks[spotify_tracks['release_date'].dt.year >= 1990]\n",
    "\n",
    "# change duration from ms to minutes\n",
    "spotify_tracks['duration_ms'] = spotify_tracks['duration_ms']/60000\n",
    "\n",
    "# rearrange columns\n",
    "spotify_tracks = spotify_tracks[['id',\n",
    "        'name',\n",
    "        'artists',\n",
    " 'id_artists',\n",
    " 'release_date',\n",
    " 'duration_ms',\n",
    " 'explicit',\n",
    " 'danceability',\n",
    " 'energy',\n",
    " 'key',\n",
    " 'loudness',\n",
    " 'mode',\n",
    " 'speechiness',\n",
    " 'acousticness',\n",
    " 'instrumentalness',\n",
    " 'liveness',\n",
    " 'valence',\n",
    " 'tempo',\n",
    " 'time_signature',\n",
    " 'popularity',\n",
    "]]\n",
    "\n",
    "# reset index\n",
    "spotify_tracks = spotify_tracks.reset_index(drop=True)\n",
    "\n",
    "# identify IQR for duration and remove outliers\n",
    "Q1 = np.percentile(spotify_tracks['duration_ms'], 25,\n",
    "                   method = 'midpoint')\n",
    "Q3 = np.percentile(spotify_tracks['duration_ms'], 75,\n",
    "                   method = 'midpoint')\n",
    "IQR = Q3 - Q1\n",
    "upper = Q3 + 1.5*IQR\n",
    "lower = Q1 - 1.5*IQR\n",
    "upper_array=np.where(spotify_tracks['duration_ms']>=upper)\n",
    "lower_array=np.where(spotify_tracks['duration_ms']<=lower)\n",
    "\n",
    "spotify_tracks.drop(upper_array[0],inplace=True)\n",
    "spotify_tracks.drop(lower_array[0],inplace=True)\n",
    "\n",
    "# remove songs with time signature = 0, 1\n",
    "spotify_tracks = spotify_tracks[(spotify_tracks['time_signature'] != 0) & \n",
    "                                (spotify_tracks['time_signature'] !=1)]\n",
    "\n",
    "# remove songs with high speechiness like talk shows, audio books, poetry\n",
    "spotify_tracks = spotify_tracks[spotify_tracks['speechiness']<0.8]\n",
    "\n",
    "# remove songs with live audiences\n",
    "spotify_tracks = spotify_tracks[spotify_tracks['liveness']<0.9]\n",
    "\n",
    "# drop the artist_id, since we have the artist name\n",
    "spotify_tracks.drop(columns = ['id', 'id_artists'], inplace=True)\n",
    "\n",
    "# drop all null values\n",
    "spotify_tracks = spotify_tracks.dropna()\n",
    "\n",
    "# separate releasedate to month and year and drop releasedate\n",
    "spotify_tracks['month'] = pd.DatetimeIndex(spotify_tracks['release_date']).month\n",
    "spotify_tracks['year'] = pd.DatetimeIndex(spotify_tracks['release_date']).year\n",
    "spotify_tracks.drop(columns = ['release_date'], axis = 1, inplace=True)\n",
    "\n",
    "# it seems like energy/loudness, as well as loudness/acousticness are correlated, and energy/acousticness; decide to remove acousticness and loudness\n",
    "spotify_tracks.drop(columns = ['loudness', 'acousticness'], inplace=True)\n",
    "\n",
    "# ensure that song name and artist name is a string\n",
    "spotify_tracks['name'] = spotify_tracks['name'].astype(str)\n",
    "spotify_tracks['artists'] = spotify_tracks['artists'].astype(str)\n",
    "\n",
    "# remove all non alphanumeric characters in song name and artists\n",
    "spotify_tracks['name'] = spotify_tracks['name'].replace(r'[^A-Za-z0-9\\s]+', '', regex=True)\n",
    "spotify_tracks['artists'] = spotify_tracks['artists'].replace(r'[^A-Za-z0-9\\s]+', '', regex=True)\n",
    "\n",
    "# remove extra spaces in song name and artists\n",
    "spotify_tracks['name'] = spotify_tracks['name'].replace(r'\\s\\s+', ' ', regex=True)\n",
    "spotify_tracks['artists'] = spotify_tracks['artists'].replace(r'\\s\\s+', ' ', regex=True)\n",
    "\n",
    "# remove all special characters, including punctuation\n",
    "spotify_tracks['name'] = spotify_tracks['name'].replace(r'[^\\w\\s]|_', '', regex=True)\n",
    "spotify_tracks['artists'] = spotify_tracks['artists'].replace(r'[^\\w\\s]|_', '', regex=True)\n",
    "\n",
    "# make all characters in song name and artist lowercase\n",
    "spotify_tracks['name'] = spotify_tracks.name.apply(lambda x: x.lower())\n",
    "spotify_tracks['artists'] = spotify_tracks.artists.apply(lambda x: x.lower())\n",
    "\n",
    "# length of spotify_tracks\n",
    "len(spotify_tracks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning for Spotify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "330087"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the dataset\n",
    "billboard_tracks = pd.read_csv('archive/charts.csv')\n",
    "\n",
    "# remove all fields other than song, rank, and artist\n",
    "billboard_tracks.drop(columns = ['date', 'last-week', 'peak-rank', 'weeks-on-board'], inplace=True)\n",
    "\n",
    "# ensure that song name and artist is a string\n",
    "billboard_tracks['song'] = billboard_tracks['song'].astype(str)\n",
    "billboard_tracks['artist'] = billboard_tracks['artist'].astype(str)\n",
    "\n",
    "# remove all non alphanumeric characters in song name and artist\n",
    "billboard_tracks['song'] = billboard_tracks['song'].replace(r'[^A-Za-z0-9\\s]+', '', regex=True)\n",
    "billboard_tracks['artist'] = billboard_tracks['artist'].replace(r'[^A-Za-z0-9\\s]+', '', regex=True)\n",
    "\n",
    "# remove extra spaces in song name and artist\n",
    "billboard_tracks['song'] = billboard_tracks['song'].replace(r'\\s\\s+', ' ', regex=True)\n",
    "billboard_tracks['artist'] = billboard_tracks['artist'].replace(r'\\s\\s+', ' ', regex=True)\n",
    "\n",
    "# remove all special characters, including punctuation\n",
    "billboard_tracks['song'] = billboard_tracks['song'].replace(r'[^\\w\\s]|_', '', regex=True)\n",
    "billboard_tracks['artist'] = billboard_tracks['artist'].replace(r'[^\\w\\s]|_', '', regex=True)\n",
    "\n",
    "# make all characters in song name lowercase\n",
    "billboard_tracks['song'] = billboard_tracks.song.apply(lambda x: x.lower())\n",
    "billboard_tracks['artist'] = billboard_tracks.artist.apply(lambda x: x.lower())\n",
    "\n",
    "# length of billboard_tracks\n",
    "len(billboard_tracks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining the two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure that columns we join on are the same\n",
    "spotify_tracks.rename(columns={'artists': 'artist', 'name': 'song'}, inplace=True)\n",
    "\n",
    "# perform left join\n",
    "combined_tracks = spotify_tracks.merge(billboard_tracks, how = 'left', on = ['song', 'artist'])\n",
    "\n",
    "# replace nan values with zero, if there is no matches from the merge\n",
    "combined_tracks['rank'] = combined_tracks['rank'].replace(np.nan, 0)\n",
    "\n",
    "# convert the rank into binary variable (1 if popular, 0 otherwise)\n",
    "combined_tracks['billboard_popularity'] = np.where(combined_tracks['rank'] > 0, 1, 0)\n",
    "\n",
    "# drop the billboard rank, since we don't want it infuencing our prediction\n",
    "combined_tracks.drop(columns = ['rank'], inplace=True)\n",
    "\n",
    "# drop the artist column, since it was only used for joining\n",
    "combined_tracks.drop(columns=['artist'], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create separate dataset with song names vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# perform count vectorizer (goal is to see if song name has impact on popularity)\n",
    "\n",
    "count_vect = TfidfVectorizer(binary=False, min_df=10)\n",
    "#print(\"init TF idf vectorizer\")\n",
    "name_vectorized = count_vect.fit_transform(combined_tracks['song'])\n",
    "\n",
    "#print(\"created vectorized song names \")\n",
    "\n",
    "# drop the song name column and add the new vectorized song name\n",
    "combined_vectsongs = combined_tracks.drop('song', axis = 1)\n",
    "\n",
    "#print(\" dropped song name column\")\n",
    "\n",
    "count_vect_df = pd.DataFrame(name_vectorized.todense(), columns = count_vect.get_feature_names_out())\n",
    "\n",
    "#print(\"created dataframe from vectorized song names\")\n",
    "\n",
    "# reset index for combined_vectsongs, count_vect_df\n",
    "#print(\"vectsongs shape: \")\n",
    "#print(combined_vectsongs.shape)\n",
    "#print(\" count vect df shape: \")\n",
    "#print(count_vect_df.shape)\n",
    "\n",
    "#combined_vectsongs = combined_vectsongs.reset_index().drop('index', axis = 1)\n",
    "# alternative way of dropping index column\n",
    "combined_vectsongs.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#count_vect_df = count_vect_df.reset_index().drop('index', axis = 1)\n",
    "count_vect_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#print(\"combined_vectsongs and count_vect_df reset index\")\n",
    "\n",
    "#combined_vectsongs = pd.concat([combined_vectsongs.reset_index().drop('index', axis = 1), count_vect_df.reset_index().drop('index', axis = 1)], axis = 1)\n",
    "\n",
    "#combined_vectsongs = pd.concat([combined_vectsongs, count_vect_df], axis = 1, ignore_index=True)\n",
    "\n",
    "#combined_vectsongs = combined_vectsongs.join(count_vect_df, lsuffix=\"_left\", rsuffix=\"_right\")\n",
    "\n",
    "combined_vectsongs = dd.from_pandas(combined_vectsongs, npartitions=8)\n",
    "count_vect_df = dd.from_pandas(count_vect_df, npartitions=8)\n",
    "\n",
    "combined_vectsongs = dd.concat([combined_vectsongs, count_vect_df], axis = 1)\n",
    "\n",
    "#print(\"concatenated the two dataframes\")\n",
    "# drop the song name from combined dataset as well\n",
    "combined_tracks.drop(columns = ['song'], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating two datasets, two where Spotify popularity is used as target, and the other two where Billboard popularity is used as target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of songs in dataset:  364000\n"
     ]
    }
   ],
   "source": [
    "# create dataset where Spotify popularity is target\n",
    "combined_spotify = combined_tracks.drop(columns = ['billboard_popularity'])\n",
    "combined_spotify_vectsongs = combined_vectsongs.drop(columns = ['billboard_popularity'])\n",
    "\n",
    "print(\"Number of songs in dataset: \", len(combined_spotify))\n",
    "\n",
    "# create dataset where Billboard popularity is target\n",
    "combined_billboard = combined_tracks.drop(columns = ['popularity'])\n",
    "combined_billboard_vectsongs = combined_vectsongs.drop(columns = ['popularity'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model 1 - Linear Regression with Spotify Popularity without song name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score for test set is 0.13699318831608898\n"
     ]
    }
   ],
   "source": [
    "# split the data into training and test sets\n",
    "df_train, df_test = train_test_split(combined_spotify, test_size = 0.2)\n",
    "\n",
    "# create the features and target dataframes\n",
    "df_train_x = df_train.drop('popularity', axis = 1).to_numpy()\n",
    "df_train_y = df_train['popularity'].values\n",
    "\n",
    "df_test_x = df_test.drop('popularity', axis = 1).to_numpy()\n",
    "df_test_y = df_test['popularity'].values\n",
    "\n",
    "# fit the linear regression model\n",
    "LinReg = LinearRegression()\n",
    "LinReg.fit(df_train_x, df_train_y)\n",
    "\n",
    "# get score on test-set\n",
    "test_score = LinReg.score(df_test_x, df_test_y)\n",
    "\n",
    "# print the score\n",
    "print(f\"R2 score for test set is {test_score}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model 2 - Linear Regression with Spotify Popularity with vectorized song name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages/dask_ml/model_selection/_split.py:462: FutureWarning: The default value for 'shuffle' must be specified when splitting DataFrames. In the future DataFrames will automatically be shuffled within blocks prior to splitting. Specify 'shuffle=True' to adopt the future behavior now, or 'shuffle=False' to retain the previous behavior.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train test split done\n",
      "df_train_x created\n",
      "df_train_y created\n",
      "df_test_x created\n",
      "df_test_y created\n",
      "Linear regression model created\n",
      "type of df_train_x:  <class 'dask.array.core.Array'>\n",
      "type of df_train_y:  <class 'dask.array.core.Array'>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtype of df_train_x: \u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mtype\u001b[39m(df_train_x))\n\u001b[1;32m     32\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtype of df_train_y: \u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mtype\u001b[39m(df_train_y))\n\u001b[0;32m---> 33\u001b[0m LinReg\u001b[39m.\u001b[39;49mfit(df_train_x, df_train_y)\n\u001b[1;32m     34\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLinear regression model fitted\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[39m# get score on test-set\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/dask_ml/linear_model/glm.py:188\u001b[0m, in \u001b[0;36m_GLM.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    184\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_array(X)\n\u001b[1;32m    186\u001b[0m solver_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_solver_kwargs()\n\u001b[0;32m--> 188\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_coef \u001b[39m=\u001b[39m algorithms\u001b[39m.\u001b[39;49m_solvers[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msolver](X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msolver_kwargs)\n\u001b[1;32m    189\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_intercept:\n\u001b[1;32m    190\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoef_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_coef[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/dask_glm/utils.py:17\u001b[0m, in \u001b[0;36mnormalize.<locals>.normalize_inputs\u001b[0;34m(X, y, *args, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m normalize \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mnormalize\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m \u001b[39mif\u001b[39;00m normalize:\n\u001b[0;32m---> 17\u001b[0m     mean, std \u001b[39m=\u001b[39m da\u001b[39m.\u001b[39;49mcompute(X\u001b[39m.\u001b[39;49mmean(axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m), X\u001b[39m.\u001b[39;49mstd(axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m))\n\u001b[1;32m     18\u001b[0m     mean, std \u001b[39m=\u001b[39m mean\u001b[39m.\u001b[39mcopy(), std\u001b[39m.\u001b[39mcopy()  \u001b[39m# in case they are read-only\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     intercept_idx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(std \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/dask/base.py:599\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    596\u001b[0m     keys\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_keys__())\n\u001b[1;32m    597\u001b[0m     postcomputes\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m--> 599\u001b[0m results \u001b[39m=\u001b[39m schedule(dsk, keys, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    600\u001b[0m \u001b[39mreturn\u001b[39;00m repack([f(r, \u001b[39m*\u001b[39ma) \u001b[39mfor\u001b[39;00m r, (f, a) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/dask/threaded.py:89\u001b[0m, in \u001b[0;36mget\u001b[0;34m(dsk, keys, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(pool, multiprocessing\u001b[39m.\u001b[39mpool\u001b[39m.\u001b[39mPool):\n\u001b[1;32m     87\u001b[0m         pool \u001b[39m=\u001b[39m MultiprocessingPoolExecutor(pool)\n\u001b[0;32m---> 89\u001b[0m results \u001b[39m=\u001b[39m get_async(\n\u001b[1;32m     90\u001b[0m     pool\u001b[39m.\u001b[39;49msubmit,\n\u001b[1;32m     91\u001b[0m     pool\u001b[39m.\u001b[39;49m_max_workers,\n\u001b[1;32m     92\u001b[0m     dsk,\n\u001b[1;32m     93\u001b[0m     keys,\n\u001b[1;32m     94\u001b[0m     cache\u001b[39m=\u001b[39;49mcache,\n\u001b[1;32m     95\u001b[0m     get_id\u001b[39m=\u001b[39;49m_thread_get_id,\n\u001b[1;32m     96\u001b[0m     pack_exception\u001b[39m=\u001b[39;49mpack_exception,\n\u001b[1;32m     97\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m     98\u001b[0m )\n\u001b[1;32m    100\u001b[0m \u001b[39m# Cleanup pools associated to dead threads\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[39mwith\u001b[39;00m pools_lock:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/dask/local.py:500\u001b[0m, in \u001b[0;36mget_async\u001b[0;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[39mwhile\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mwaiting\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mor\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mready\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mor\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mrunning\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    499\u001b[0m     fire_tasks(chunksize)\n\u001b[0;32m--> 500\u001b[0m     \u001b[39mfor\u001b[39;00m key, res_info, failed \u001b[39min\u001b[39;00m queue_get(queue)\u001b[39m.\u001b[39mresult():\n\u001b[1;32m    501\u001b[0m         \u001b[39mif\u001b[39;00m failed:\n\u001b[1;32m    502\u001b[0m             exc, tb \u001b[39m=\u001b[39m loads(res_info)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/dask/local.py:137\u001b[0m, in \u001b[0;36mqueue_get\u001b[0;34m(q)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mqueue_get\u001b[39m(q):\n\u001b[0;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m q\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnot_empty\u001b[39m.\u001b[39;49mwait()\n\u001b[1;32m    172\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be a non-negative number\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    313\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# split the data into training and test sets\n",
    "import dask_ml.model_selection as dcv\n",
    "import dask_ml.linear_model as dlm\n",
    "\n",
    "#print(\"memory usage of combined_spotify_vectsongs:\")\n",
    "#print(combined_spotify_vectsongs.memory_usage(deep=True).sum())\n",
    "#combined_spotify_vectsongs = combined_spotify_vectsongs.compute() \n",
    "#print(\"shape of combined_spotify_vectsongs: \")\n",
    "#print(combined_spotify_vectsongs.shape)\n",
    "#print(\"type of combined_spotify_vectsongs: \")\n",
    "#print(type(combined_spotify_vectsongs))\n",
    "df_train, df_test = dcv.train_test_split(combined_spotify_vectsongs, test_size = 0.2)\n",
    "print(\"train test split done\")\n",
    "\n",
    "# create the features and target dataframes\n",
    "#df_train_x = df_train.drop('popularity', axis = 1).to_numpy()\n",
    "df_train_x = df_train.drop('popularity', axis = 1).values\n",
    "print(\"df_train_x created\")\n",
    "df_train_y = df_train['popularity'].values\n",
    "print(\"df_train_y created\")\n",
    "\n",
    "#df_test_x = df_test.drop('popularity', axis = 1).to_numpy()\n",
    "df_test_x = df_test.drop('popularity', axis = 1).values\n",
    "print(\"df_test_x created\")\n",
    "df_test_y = df_test['popularity'].values\n",
    "print(\"df_test_y created\")\n",
    "\n",
    "# fit the linear regression model\n",
    "LinReg = dlm.LinearRegression(n_jobs=4)\n",
    "print(\"Linear regression model created\")\n",
    "#print(\"type of df_train_x: \", type(df_train_x))\n",
    "#print(\"type of df_train_y: \", type(df_train_y))\n",
    "LinReg.fit(df_train_x, df_train_y)\n",
    "print(\"Linear regression model fitted\")\n",
    "\n",
    "# get score on test-set\n",
    "test_score = LinReg.score(df_test_x, df_test_y)\n",
    "\n",
    "# print the score\n",
    "print(f\"R2 score for test set is {test_score}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model 3 - Logistic Regression with Billboard Popularity without song name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set with no regularization terms F1-Score is 0.7097802197802198\n",
      "Test set with no regularization terms F1-Score is 0.708708791208791\n"
     ]
    }
   ],
   "source": [
    "# split the data into training and test sets\n",
    "df_train, df_test = train_test_split(combined_billboard, test_size = 0.2)\n",
    "\n",
    "# create the features and target dataframes\n",
    "df_train_x = df_train.drop('billboard_popularity', axis = 1).to_numpy()\n",
    "df_train_y = df_train['billboard_popularity'].values\n",
    "\n",
    "df_test_x = df_test.drop('billboard_popularity', axis = 1).to_numpy()\n",
    "df_test_y = df_test['billboard_popularity'].values\n",
    "\n",
    "# fit the logistic regression model with no regularization terms\n",
    "LogReg = LogisticRegression(multi_class='ovr', penalty='none', max_iter = 10000)\n",
    "LogReg.fit(df_train_x, df_train_y)\n",
    "\n",
    "# calculate F1 score\n",
    "f1_train = f1_score(df_train_y, LogReg.predict(df_train_x), average = 'micro')\n",
    "f1_test = f1_score(df_test_y, LogReg.predict(df_test_x), average = 'micro')\n",
    "\n",
    "# print F1 values out\n",
    "print(f\"Training set with no regularization terms F1-Score is {f1_train}\")\n",
    "print(f\"Test set with no regularization terms F1-Score is {f1_test}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model 4 - Logistic Regression with Billboard Popularity with vectorized song name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set with no regularization terms F1-Score is 0.8127026098901099\n",
      "Test set with no regularization terms F1-Score is 0.8119093406593406\n"
     ]
    }
   ],
   "source": [
    "# split the data into training and test sets\n",
    "df_train, df_test = train_test_split(combined_billboard_vectsongs, test_size = 0.2)\n",
    "\n",
    "# create the features and target dataframes\n",
    "df_train_x = df_train.drop('billboard_popularity', axis = 1).to_numpy()\n",
    "df_train_y = df_train['billboard_popularity'].values\n",
    "\n",
    "df_test_x = df_test.drop('billboard_popularity', axis = 1).to_numpy()\n",
    "df_test_y = df_test['billboard_popularity'].values\n",
    "\n",
    "# fit the logistic regression model with no regularization terms\n",
    "LogReg = LogisticRegression(multi_class='ovr', penalty='none', max_iter = 10000)\n",
    "LogReg.fit(df_train_x, df_train_y)\n",
    "\n",
    "# calculate F1 score\n",
    "f1_train = f1_score(df_train_y, LogReg.predict(df_train_x), average = 'micro')\n",
    "f1_test = f1_score(df_test_y, LogReg.predict(df_test_x), average = 'micro')\n",
    "\n",
    "# print F1 values out\n",
    "print(f\"Training set with no regularization terms F1-Score is {f1_train}\")\n",
    "print(f\"Test set with no regularization terms F1-Score is {f1_test}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
