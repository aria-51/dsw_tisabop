{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: dask[complete] in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (2023.4.1)\n",
      "Requirement already satisfied: click>=8.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[complete]) (8.1.3)\n",
      "Requirement already satisfied: partd>=1.2.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[complete]) (1.4.0)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[complete]) (0.12.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[complete]) (6.0)\n",
      "Requirement already satisfied: cloudpickle>=1.5.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[complete]) (2.2.1)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[complete]) (2023.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[complete]) (23.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[complete]) (6.3.0)\n",
      "Requirement already satisfied: lz4>=4.3.2 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[complete]) (4.3.2)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[complete]) (12.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.13.0->dask[complete]) (3.15.0)\n",
      "Requirement already satisfied: locket in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from partd>=1.2.0->dask[complete]) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from pyarrow>=7.0->dask[complete]) (1.24.3)\n",
      "Requirement already satisfied: bokeh>=2.4.2 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[complete]) (3.1.1)\n",
      "Requirement already satisfied: jinja2>=2.10.3 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[complete]) (3.1.2)\n",
      "Requirement already satisfied: distributed==2023.4.1 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[complete]) (2023.4.1)\n",
      "Requirement already satisfied: pandas>=1.3 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[complete]) (2.0.1)\n",
      "Requirement already satisfied: msgpack>=1.0.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed==2023.4.1->dask[complete]) (1.0.5)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.5 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed==2023.4.1->dask[complete]) (2.4.0)\n",
      "Requirement already satisfied: zict>=2.2.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed==2023.4.1->dask[complete]) (3.0.0)\n",
      "Requirement already satisfied: tornado>=6.0.3 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed==2023.4.1->dask[complete]) (6.2)\n",
      "Requirement already satisfied: psutil>=5.7.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed==2023.4.1->dask[complete]) (5.9.4)\n",
      "Requirement already satisfied: urllib3>=1.24.3 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed==2023.4.1->dask[complete]) (2.0.2)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed==2023.4.1->dask[complete]) (1.7.0)\n",
      "Requirement already satisfied: xyzservices>=2021.09.1 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from bokeh>=2.4.2->dask[complete]) (2023.2.0)\n",
      "Requirement already satisfied: pillow>=7.1.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from bokeh>=2.4.2->dask[complete]) (9.5.0)\n",
      "Requirement already satisfied: contourpy>=1 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from bokeh>=2.4.2->dask[complete]) (1.0.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from jinja2>=2.10.3->dask[complete]) (2.1.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from pandas>=1.3->dask[complete]) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from pandas>=1.3->dask[complete]) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from pandas>=1.3->dask[complete]) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from python-dateutil>=2.8.2->pandas>=1.3->dask[complete]) (1.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install \"dask[complete]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: dask-ml in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (2023.3.24)\n",
      "Requirement already satisfied: packaging in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask-ml) (23.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask-ml) (1.24.3)\n",
      "Requirement already satisfied: scipy in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask-ml) (1.10.1)\n",
      "Requirement already satisfied: multipledispatch>=0.4.9 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask-ml) (0.6.0)\n",
      "Requirement already satisfied: scikit-learn>=1.2.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask-ml) (1.2.2)\n",
      "Requirement already satisfied: numba>=0.51.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask-ml) (0.57.0)\n",
      "Requirement already satisfied: distributed>=2.4.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask-ml) (2023.4.1)\n",
      "Requirement already satisfied: dask-glm>=0.2.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask-ml) (0.2.0)\n",
      "Requirement already satisfied: pandas>=0.24.2 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask-ml) (2.0.1)\n",
      "Requirement already satisfied: dask[array,dataframe]>=2.4.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask-ml) (2023.4.1)\n",
      "Requirement already satisfied: cloudpickle>=0.2.2 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask-glm>=0.2.0->dask-ml) (2.2.1)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (6.0)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (2023.5.0)\n",
      "Requirement already satisfied: click>=8.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (8.1.3)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (0.12.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (6.3.0)\n",
      "Requirement already satisfied: partd>=1.2.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (1.4.0)\n",
      "Requirement already satisfied: psutil>=5.7.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed>=2.4.0->dask-ml) (5.9.4)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed>=2.4.0->dask-ml) (1.7.0)\n",
      "Requirement already satisfied: urllib3>=1.24.3 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed>=2.4.0->dask-ml) (2.0.2)\n",
      "Requirement already satisfied: locket>=1.0.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed>=2.4.0->dask-ml) (1.0.0)\n",
      "Requirement already satisfied: jinja2>=2.10.3 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed>=2.4.0->dask-ml) (3.1.2)\n",
      "Requirement already satisfied: zict>=2.2.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed>=2.4.0->dask-ml) (3.0.0)\n",
      "Requirement already satisfied: msgpack>=1.0.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed>=2.4.0->dask-ml) (1.0.5)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.5 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed>=2.4.0->dask-ml) (2.4.0)\n",
      "Requirement already satisfied: tornado>=6.0.3 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed>=2.4.0->dask-ml) (6.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.13.0->dask[array,dataframe]>=2.4.0->dask-ml) (3.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from jinja2>=2.10.3->distributed>=2.4.0->dask-ml) (2.1.2)\n",
      "Requirement already satisfied: six in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from multipledispatch>=0.4.9->dask-ml) (1.16.0)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from numba>=0.51.0->dask-ml) (0.40.0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from pandas>=0.24.2->dask-ml) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from pandas>=0.24.2->dask-ml) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from pandas>=0.24.2->dask-ml) (2.8.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from scikit-learn>=1.2.0->dask-ml) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from scikit-learn>=1.2.0->dask-ml) (3.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install dask-ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.24.3'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning for Spotify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265669"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import relevant libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# import the dataset\n",
    "spotify_tracks = pd.read_csv('archive/tracks.csv')\n",
    "\n",
    "# drop duplicates with the same name and artist\n",
    "spotify_tracks = spotify_tracks.drop_duplicates(\n",
    "  subset = ['name', 'artists'],\n",
    "  keep = 'last').reset_index(drop = True)\n",
    "\n",
    "# convert release_time to appropriate time date format\n",
    "spotify_tracks['release_date']= pd.to_datetime(spotify_tracks['release_date'], format='mixed')\n",
    "\n",
    "# remove songs older than 1990\n",
    "spotify_tracks = spotify_tracks[spotify_tracks['release_date'].dt.year >= 1990]\n",
    "\n",
    "# change duration from ms to minutes\n",
    "spotify_tracks['duration_ms'] = spotify_tracks['duration_ms']/60000\n",
    "\n",
    "# rearrange columns\n",
    "spotify_tracks = spotify_tracks[['id',\n",
    "        'name',\n",
    "        'artists',\n",
    " 'id_artists',\n",
    " 'release_date',\n",
    " 'duration_ms',\n",
    " 'explicit',\n",
    " 'danceability',\n",
    " 'energy',\n",
    " 'key',\n",
    " 'loudness',\n",
    " 'mode',\n",
    " 'speechiness',\n",
    " 'acousticness',\n",
    " 'instrumentalness',\n",
    " 'liveness',\n",
    " 'valence',\n",
    " 'tempo',\n",
    " 'time_signature',\n",
    " 'popularity',\n",
    "]]\n",
    "\n",
    "# reset index\n",
    "spotify_tracks = spotify_tracks.reset_index(drop=True)\n",
    "\n",
    "# identify IQR for duration and remove outliers\n",
    "Q1 = np.percentile(spotify_tracks['duration_ms'], 25,\n",
    "                   method = 'midpoint')\n",
    "Q3 = np.percentile(spotify_tracks['duration_ms'], 75,\n",
    "                   method = 'midpoint')\n",
    "IQR = Q3 - Q1\n",
    "upper = Q3 + 1.5*IQR\n",
    "lower = Q1 - 1.5*IQR\n",
    "upper_array=np.where(spotify_tracks['duration_ms']>=upper)\n",
    "lower_array=np.where(spotify_tracks['duration_ms']<=lower)\n",
    "\n",
    "spotify_tracks.drop(upper_array[0],inplace=True)\n",
    "spotify_tracks.drop(lower_array[0],inplace=True)\n",
    "\n",
    "# remove songs with time signature = 0, 1\n",
    "spotify_tracks = spotify_tracks[(spotify_tracks['time_signature'] != 0) & \n",
    "                                (spotify_tracks['time_signature'] !=1)]\n",
    "\n",
    "# remove songs with high speechiness like talk shows, audio books, poetry\n",
    "spotify_tracks = spotify_tracks[spotify_tracks['speechiness']<0.8]\n",
    "\n",
    "# remove songs with live audiences\n",
    "spotify_tracks = spotify_tracks[spotify_tracks['liveness']<0.9]\n",
    "\n",
    "# drop the artist_id, since we have the artist name\n",
    "spotify_tracks.drop(columns = ['id', 'id_artists'], inplace=True)\n",
    "\n",
    "# drop all null values\n",
    "spotify_tracks = spotify_tracks.dropna()\n",
    "\n",
    "# separate releasedate to month and year and drop releasedate\n",
    "spotify_tracks['month'] = pd.DatetimeIndex(spotify_tracks['release_date']).month\n",
    "spotify_tracks['year'] = pd.DatetimeIndex(spotify_tracks['release_date']).year\n",
    "spotify_tracks.drop(columns = ['release_date'], axis = 1, inplace=True)\n",
    "\n",
    "# it seems like energy/loudness, as well as loudness/acousticness are correlated, and energy/acousticness; decide to remove acousticness and loudness\n",
    "spotify_tracks.drop(columns = ['loudness', 'acousticness'], inplace=True)\n",
    "\n",
    "# ensure that song name and artist name is a string\n",
    "spotify_tracks['name'] = spotify_tracks['name'].astype(str)\n",
    "spotify_tracks['artists'] = spotify_tracks['artists'].astype(str)\n",
    "\n",
    "# remove all non alphanumeric characters in song name and artists\n",
    "spotify_tracks['name'] = spotify_tracks['name'].replace(r'[^A-Za-z0-9\\s]+', '', regex=True)\n",
    "spotify_tracks['artists'] = spotify_tracks['artists'].replace(r'[^A-Za-z0-9\\s]+', '', regex=True)\n",
    "\n",
    "# remove extra spaces in song name and artists\n",
    "spotify_tracks['name'] = spotify_tracks['name'].replace(r'\\s\\s+', ' ', regex=True)\n",
    "spotify_tracks['artists'] = spotify_tracks['artists'].replace(r'\\s\\s+', ' ', regex=True)\n",
    "\n",
    "# remove all special characters, including punctuation\n",
    "spotify_tracks['name'] = spotify_tracks['name'].replace(r'[^\\w\\s]|_', '', regex=True)\n",
    "spotify_tracks['artists'] = spotify_tracks['artists'].replace(r'[^\\w\\s]|_', '', regex=True)\n",
    "\n",
    "# make all characters in song name and artist lowercase\n",
    "spotify_tracks['name'] = spotify_tracks.name.apply(lambda x: x.lower())\n",
    "spotify_tracks['artists'] = spotify_tracks.artists.apply(lambda x: x.lower())\n",
    "\n",
    "# length of spotify_tracks\n",
    "len(spotify_tracks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning for Spotify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "330087"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the dataset\n",
    "billboard_tracks = pd.read_csv('archive/charts.csv')\n",
    "\n",
    "# remove all fields other than song, rank, and artist\n",
    "billboard_tracks.drop(columns = ['date', 'last-week', 'peak-rank', 'weeks-on-board'], inplace=True)\n",
    "\n",
    "# ensure that song name and artist is a string\n",
    "billboard_tracks['song'] = billboard_tracks['song'].astype(str)\n",
    "billboard_tracks['artist'] = billboard_tracks['artist'].astype(str)\n",
    "\n",
    "# remove all non alphanumeric characters in song name and artist\n",
    "billboard_tracks['song'] = billboard_tracks['song'].replace(r'[^A-Za-z0-9\\s]+', '', regex=True)\n",
    "billboard_tracks['artist'] = billboard_tracks['artist'].replace(r'[^A-Za-z0-9\\s]+', '', regex=True)\n",
    "\n",
    "# remove extra spaces in song name and artist\n",
    "billboard_tracks['song'] = billboard_tracks['song'].replace(r'\\s\\s+', ' ', regex=True)\n",
    "billboard_tracks['artist'] = billboard_tracks['artist'].replace(r'\\s\\s+', ' ', regex=True)\n",
    "\n",
    "# remove all special characters, including punctuation\n",
    "billboard_tracks['song'] = billboard_tracks['song'].replace(r'[^\\w\\s]|_', '', regex=True)\n",
    "billboard_tracks['artist'] = billboard_tracks['artist'].replace(r'[^\\w\\s]|_', '', regex=True)\n",
    "\n",
    "# make all characters in song name lowercase\n",
    "billboard_tracks['song'] = billboard_tracks.song.apply(lambda x: x.lower())\n",
    "billboard_tracks['artist'] = billboard_tracks.artist.apply(lambda x: x.lower())\n",
    "\n",
    "# length of billboard_tracks\n",
    "len(billboard_tracks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining the two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure that columns we join on are the same\n",
    "spotify_tracks.rename(columns={'artists': 'artist', 'name': 'song'}, inplace=True)\n",
    "\n",
    "# perform left join\n",
    "combined_tracks = spotify_tracks.merge(billboard_tracks, how = 'left', on = ['song', 'artist'])\n",
    "\n",
    "# replace nan values with zero, if there is no matches from the merge\n",
    "combined_tracks['rank'] = combined_tracks['rank'].replace(np.nan, 0)\n",
    "\n",
    "# convert the rank into binary variable (1 if popular, 0 otherwise)\n",
    "combined_tracks['billboard_popularity'] = np.where(combined_tracks['rank'] > 0, 1, 0)\n",
    "\n",
    "# drop the billboard rank, since we don't want it infuencing our prediction\n",
    "combined_tracks.drop(columns = ['rank'], inplace=True)\n",
    "\n",
    "# drop the artist column, since it was only used for joining\n",
    "combined_tracks.drop(columns=['artist'], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create separate dataset with song names vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of combined_vectsongs: \n",
      "(364000, 16)\n",
      "shape of count_vect_df: \n",
      "(364000, 753)\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# perform count vectorizer (goal is to see if song name has impact on popularity)\n",
    "\n",
    "count_vect = TfidfVectorizer(binary=False, min_df=150)\n",
    "#print(\"init TF idf vectorizer\")\n",
    "name_vectorized = count_vect.fit_transform(combined_tracks['song'])\n",
    "\n",
    "# drop the song name column and add the new vectorized song name\n",
    "combined_vectsongs = combined_tracks.drop('song', axis = 1)\n",
    "\n",
    "count_vect_df = pd.DataFrame(name_vectorized.todense(), columns = count_vect.get_feature_names_out())\n",
    "\n",
    "# alternative way of dropping index column\n",
    "combined_vectsongs.reset_index(drop=True, inplace=True)\n",
    "print(\"shape of combined_vectsongs: \")\n",
    "print(combined_vectsongs.shape)\n",
    "\n",
    "#count_vect_df = count_vect_df.reset_index().drop('index', axis = 1)\n",
    "count_vect_df.reset_index(drop=True, inplace=True)\n",
    "print(\"shape of count_vect_df: \")\n",
    "print(count_vect_df.shape)\n",
    "\n",
    "combined_vectsongs = pd.concat([combined_vectsongs, count_vect_df], axis = 1)\n",
    "\n",
    "# drop the song name from combined dataset as well\n",
    "combined_tracks.drop(columns = ['song'], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating two datasets, two where Spotify popularity is used as target, and the other two where Billboard popularity is used as target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of songs in dataset:  364000\n"
     ]
    }
   ],
   "source": [
    "# create dataset where Spotify popularity is target\n",
    "combined_spotify = combined_tracks.drop(columns = ['billboard_popularity'])\n",
    "combined_spotify_vectsongs = combined_vectsongs.drop(columns = ['billboard_popularity'])\n",
    "\n",
    "print(\"Number of songs in dataset: \", len(combined_spotify))\n",
    "\n",
    "# create dataset where Billboard popularity is target\n",
    "combined_billboard = combined_tracks.drop(columns = ['popularity'])\n",
    "combined_billboard_vectsongs = combined_vectsongs.drop(columns = ['popularity'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model 1 - Linear Regression with Spotify Popularity without song name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score for test set is 0.13833749358581326\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# standardize split the data into training and test sets\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df_train, df_test = train_test_split(combined_spotify, test_size = 0.2)\n",
    "\n",
    "# create the features and target dataframes\n",
    "df_train_x = df_train.drop('popularity', axis = 1).to_numpy()\n",
    "df_train_y = df_train['popularity'].values\n",
    "df_train_x = scaler.fit_transform(df_train_x)\n",
    "\n",
    "df_test_x = df_test.drop('popularity', axis = 1).to_numpy()\n",
    "df_test_y = df_test['popularity'].values\n",
    "df_test_x = scaler.fit_transform(df_test_x)\n",
    "\n",
    "# fit the linear regression model\n",
    "LinReg = LinearRegression()\n",
    "LinReg.fit(df_train_x, df_train_y)\n",
    "\n",
    "# get score on test-set\n",
    "test_score = LinReg.score(df_test_x, df_test_y)\n",
    "\n",
    "# print the score\n",
    "print(f\"R2 score for test set is {test_score}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model 2 - Linear Regression with Spotify Popularity with vectorized song name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train test split done\n",
      "df_train_x created\n",
      "df_train_y created:\n",
      "[19  0 56 ...  5 43 24]\n",
      "df_train_x scaled\n",
      "df_test_x created\n",
      "df_test_y created\n",
      "Linear regression model created\n",
      "type of df_train_y:  <class 'numpy.ndarray'>\n",
      "df_train_x shape:  (291200, 767)\n",
      "df_train_y shape:  (291200,)\n",
      "Linear regression model fitted\n",
      "R2 score for test set is 0.28644558358294825\n"
     ]
    }
   ],
   "source": [
    "# split the data into training and test sets\n",
    "import dask_ml.model_selection as dcv\n",
    "import dask_ml.linear_model as dlm\n",
    "import dask_ml.preprocessing as dpp\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_train, df_test = train_test_split(combined_spotify_vectsongs, test_size = 0.2)\n",
    "\n",
    "# create the features and target dataframes\n",
    "df_train_x = df_train.drop('popularity', axis = 1).values\n",
    "df_train_y = df_train['popularity'].values\n",
    "df_train_x = scaler.fit_transform(df_train_x)\n",
    "\n",
    "#df_test_x = df_test.drop('popularity', axis = 1).to_numpy()\n",
    "df_test_x = df_test.drop('popularity', axis = 1).values\n",
    "df_test_y = df_test['popularity'].values\n",
    "df_test_x = scaler.fit_transform(df_test_x)\n",
    "\n",
    "# fit the linear regression model\n",
    "LinReg = LinearRegression(n_jobs=8)\n",
    "LinReg.fit(df_train_x, df_train_y)\n",
    "\n",
    "# get score on test-set\n",
    "test_score = LinReg.score(df_test_x, df_test_y)\n",
    "\n",
    "# print the score\n",
    "print(f\"R2 score for test set is {test_score}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model 3 - Logistic Regression with Billboard Popularity without song name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set with no regularization terms F1-Score is 0.7121565934065934\n",
      "Test set with no regularization terms F1-Score is 0.7126510989010989\n"
     ]
    }
   ],
   "source": [
    "# split the data into training and test sets\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df_train, df_test = train_test_split(combined_billboard, test_size = 0.2)\n",
    "\n",
    "# create the features and target dataframes\n",
    "df_train_x = df_train.drop('billboard_popularity', axis = 1).to_numpy()\n",
    "df_train_y = df_train['billboard_popularity'].values\n",
    "df_train_x = scaler.fit_transform(df_train_x)\n",
    "\n",
    "df_test_x = df_test.drop('billboard_popularity', axis = 1).to_numpy()\n",
    "df_test_y = df_test['billboard_popularity'].values\n",
    "df_test_x = scaler.fit_transform(df_test_x)\n",
    "\n",
    "# fit the logistic regression model with no regularization terms\n",
    "LogReg = LogisticRegression(multi_class='ovr', penalty='none', max_iter = 10000)\n",
    "LogReg.fit(df_train_x, df_train_y)\n",
    "\n",
    "# calculate F1 score\n",
    "f1_train = f1_score(df_train_y, LogReg.predict(df_train_x), average = 'micro')\n",
    "f1_test = f1_score(df_test_y, LogReg.predict(df_test_x), average = 'micro')\n",
    "\n",
    "# print F1 values out\n",
    "print(f\"Training set with no regularization terms F1-Score is {f1_train}\")\n",
    "print(f\"Test set with no regularization terms F1-Score is {f1_test}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model 4 - Logistic Regression with Billboard Popularity with vectorized song name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39m# fit the logistic regression model with no regularization terms\u001b[39;00m\n\u001b[1;32m     17\u001b[0m LogReg \u001b[39m=\u001b[39m LogisticRegression(multi_class\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39movr\u001b[39m\u001b[39m'\u001b[39m, penalty\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m'\u001b[39m, max_iter \u001b[39m=\u001b[39m \u001b[39m10000\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m LogReg\u001b[39m.\u001b[39;49mfit(df_train_x, df_train_y)\n\u001b[1;32m     20\u001b[0m \u001b[39m# calculate F1 score\u001b[39;00m\n\u001b[1;32m     21\u001b[0m f1_train \u001b[39m=\u001b[39m f1_score(df_train_y, LogReg\u001b[39m.\u001b[39mpredict(df_train_x), average \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmicro\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1291\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1288\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1289\u001b[0m     n_threads \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 1291\u001b[0m fold_coefs_ \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs, verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose, prefer\u001b[39m=\u001b[39;49mprefer)(\n\u001b[1;32m   1292\u001b[0m     path_func(\n\u001b[1;32m   1293\u001b[0m         X,\n\u001b[1;32m   1294\u001b[0m         y,\n\u001b[1;32m   1295\u001b[0m         pos_class\u001b[39m=\u001b[39;49mclass_,\n\u001b[1;32m   1296\u001b[0m         Cs\u001b[39m=\u001b[39;49m[C_],\n\u001b[1;32m   1297\u001b[0m         l1_ratio\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ml1_ratio,\n\u001b[1;32m   1298\u001b[0m         fit_intercept\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_intercept,\n\u001b[1;32m   1299\u001b[0m         tol\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtol,\n\u001b[1;32m   1300\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m   1301\u001b[0m         solver\u001b[39m=\u001b[39;49msolver,\n\u001b[1;32m   1302\u001b[0m         multi_class\u001b[39m=\u001b[39;49mmulti_class,\n\u001b[1;32m   1303\u001b[0m         max_iter\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter,\n\u001b[1;32m   1304\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[1;32m   1305\u001b[0m         check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1306\u001b[0m         random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state,\n\u001b[1;32m   1307\u001b[0m         coef\u001b[39m=\u001b[39;49mwarm_start_coef_,\n\u001b[1;32m   1308\u001b[0m         penalty\u001b[39m=\u001b[39;49mpenalty,\n\u001b[1;32m   1309\u001b[0m         max_squared_sum\u001b[39m=\u001b[39;49mmax_squared_sum,\n\u001b[1;32m   1310\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1311\u001b[0m         n_threads\u001b[39m=\u001b[39;49mn_threads,\n\u001b[1;32m   1312\u001b[0m     )\n\u001b[1;32m   1313\u001b[0m     \u001b[39mfor\u001b[39;49;00m class_, warm_start_coef_ \u001b[39min\u001b[39;49;00m \u001b[39mzip\u001b[39;49m(classes_, warm_start_coef)\n\u001b[1;32m   1314\u001b[0m )\n\u001b[1;32m   1316\u001b[0m fold_coefs_, _, n_iter_ \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mfold_coefs_)\n\u001b[1;32m   1317\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(n_iter_, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mint32)[:, \u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/joblib/parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1077\u001b[0m     \u001b[39m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[1;32m   1078\u001b[0m     \u001b[39m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[39m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[1;32m   1083\u001b[0m     \u001b[39m# remaining jobs.\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 1085\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1088\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[1;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[1;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:450\u001b[0m, in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[1;32m    446\u001b[0m l2_reg_strength \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m \u001b[39m/\u001b[39m C\n\u001b[1;32m    447\u001b[0m iprint \u001b[39m=\u001b[39m [\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m50\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m100\u001b[39m, \u001b[39m101\u001b[39m][\n\u001b[1;32m    448\u001b[0m     np\u001b[39m.\u001b[39msearchsorted(np\u001b[39m.\u001b[39marray([\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m]), verbose)\n\u001b[1;32m    449\u001b[0m ]\n\u001b[0;32m--> 450\u001b[0m opt_res \u001b[39m=\u001b[39m optimize\u001b[39m.\u001b[39;49mminimize(\n\u001b[1;32m    451\u001b[0m     func,\n\u001b[1;32m    452\u001b[0m     w0,\n\u001b[1;32m    453\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mL-BFGS-B\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    454\u001b[0m     jac\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    455\u001b[0m     args\u001b[39m=\u001b[39;49m(X, target, sample_weight, l2_reg_strength, n_threads),\n\u001b[1;32m    456\u001b[0m     options\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39miprint\u001b[39;49m\u001b[39m\"\u001b[39;49m: iprint, \u001b[39m\"\u001b[39;49m\u001b[39mgtol\u001b[39;49m\u001b[39m\"\u001b[39;49m: tol, \u001b[39m\"\u001b[39;49m\u001b[39mmaxiter\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_iter},\n\u001b[1;32m    457\u001b[0m )\n\u001b[1;32m    458\u001b[0m n_iter_i \u001b[39m=\u001b[39m _check_optimize_result(\n\u001b[1;32m    459\u001b[0m     solver,\n\u001b[1;32m    460\u001b[0m     opt_res,\n\u001b[1;32m    461\u001b[0m     max_iter,\n\u001b[1;32m    462\u001b[0m     extra_warning_msg\u001b[39m=\u001b[39m_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n\u001b[1;32m    463\u001b[0m )\n\u001b[1;32m    464\u001b[0m w0, loss \u001b[39m=\u001b[39m opt_res\u001b[39m.\u001b[39mx, opt_res\u001b[39m.\u001b[39mfun\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/scipy/optimize/_minimize.py:696\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    693\u001b[0m     res \u001b[39m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    694\u001b[0m                              \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[1;32m    695\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39ml-bfgs-b\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 696\u001b[0m     res \u001b[39m=\u001b[39m _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[1;32m    697\u001b[0m                            callback\u001b[39m=\u001b[39;49mcallback, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)\n\u001b[1;32m    698\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtnc\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    699\u001b[0m     res \u001b[39m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[39m=\u001b[39mcallback,\n\u001b[1;32m    700\u001b[0m                         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/scipy/optimize/_lbfgsb_py.py:359\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    353\u001b[0m task_str \u001b[39m=\u001b[39m task\u001b[39m.\u001b[39mtobytes()\n\u001b[1;32m    354\u001b[0m \u001b[39mif\u001b[39;00m task_str\u001b[39m.\u001b[39mstartswith(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFG\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[39m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[1;32m    356\u001b[0m     \u001b[39m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[39m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[1;32m    358\u001b[0m     \u001b[39m# Overwrite f and g:\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m     f, g \u001b[39m=\u001b[39m func_and_grad(x)\n\u001b[1;32m    360\u001b[0m \u001b[39melif\u001b[39;00m task_str\u001b[39m.\u001b[39mstartswith(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNEW_X\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    361\u001b[0m     \u001b[39m# new iteration\u001b[39;00m\n\u001b[1;32m    362\u001b[0m     n_iterations \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/scipy/optimize/_differentiable_functions.py:285\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39marray_equal(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx):\n\u001b[1;32m    284\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_x_impl(x)\n\u001b[0;32m--> 285\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun()\n\u001b[1;32m    286\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_grad()\n\u001b[1;32m    287\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/scipy/optimize/_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_fun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    250\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated:\n\u001b[0;32m--> 251\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun_impl()\n\u001b[1;32m    252\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/scipy/optimize/_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_fun\u001b[39m():\n\u001b[0;32m--> 155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39m=\u001b[39m fun_wrapped(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/scipy/optimize/_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnfev \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[39m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[39m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[39m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m fx \u001b[39m=\u001b[39m fun(np\u001b[39m.\u001b[39;49mcopy(x), \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    138\u001b[0m \u001b[39m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misscalar(fx):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/scipy/optimize/_optimize.py:76\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x, \u001b[39m*\u001b[39margs):\n\u001b[1;32m     75\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_if_needed(x, \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m     77\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/scipy/optimize/_optimize.py:70\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39mall(x \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjac \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(x)\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m---> 70\u001b[0m     fg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfun(x, \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m     71\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjac \u001b[39m=\u001b[39m fg[\u001b[39m1\u001b[39m]\n\u001b[1;32m     72\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value \u001b[39m=\u001b[39m fg[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_linear_loss.py:289\u001b[0m, in \u001b[0;36mLinearModelLoss.loss_gradient\u001b[0;34m(self, coef, X, y, sample_weight, l2_reg_strength, n_threads, raw_prediction)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_loss\u001b[39m.\u001b[39mis_multiclass:\n\u001b[1;32m    288\u001b[0m     grad \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty_like(coef, dtype\u001b[39m=\u001b[39mweights\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m--> 289\u001b[0m     grad[:n_features] \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39;49mT \u001b[39m@\u001b[39;49m grad_pointwise \u001b[39m+\u001b[39m l2_reg_strength \u001b[39m*\u001b[39m weights\n\u001b[1;32m    290\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_intercept:\n\u001b[1;32m    291\u001b[0m         grad[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m grad_pointwise\u001b[39m.\u001b[39msum()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# split the data into training and test sets\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df_train, df_test = train_test_split(combined_billboard_vectsongs, test_size = 0.2)\n",
    "\n",
    "# create the features and target dataframes\n",
    "df_train_x = df_train.drop('billboard_popularity', axis = 1).to_numpy()\n",
    "df_train_y = df_train['billboard_popularity'].values\n",
    "df_train_x = scaler.fit_transform(df_train_x)\n",
    "\n",
    "df_test_x = df_test.drop('billboard_popularity', axis = 1).to_numpy()\n",
    "df_test_y = df_test['billboard_popularity'].values\n",
    "df_test_x = scaler.fit_transform(df_test_x)\n",
    "\n",
    "# fit the logistic regression model with no regularization terms\n",
    "LogReg = LogisticRegression(multi_class='ovr', penalty='none', max_iter = 10000)\n",
    "LogReg.fit(df_train_x, df_train_y)\n",
    "\n",
    "# calculate F1 score\n",
    "f1_train = f1_score(df_train_y, LogReg.predict(df_train_x), average = 'micro')\n",
    "f1_test = f1_score(df_test_y, LogReg.predict(df_test_x), average = 'micro')\n",
    "\n",
    "# print F1 values out\n",
    "print(f\"Training set with no regularization terms F1-Score is {f1_train}\")\n",
    "print(f\"Test set with no regularization terms F1-Score is {f1_test}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
