{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: dask[complete] in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (2023.4.1)\n",
      "Requirement already satisfied: cloudpickle>=1.5.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[complete]) (2.2.1)\n",
      "Requirement already satisfied: click>=8.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[complete]) (8.1.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[complete]) (6.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[complete]) (23.0)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[complete]) (0.12.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[complete]) (6.0)\n",
      "Requirement already satisfied: partd>=1.2.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[complete]) (1.4.0)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[complete]) (2023.5.0)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[complete]) (12.0.0)\n",
      "Requirement already satisfied: lz4>=4.3.2 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[complete]) (4.3.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.13.0->dask[complete]) (3.15.0)\n",
      "Requirement already satisfied: locket in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from partd>=1.2.0->dask[complete]) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from pyarrow>=7.0->dask[complete]) (1.24.3)\n",
      "Requirement already satisfied: distributed==2023.4.1 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[complete]) (2023.4.1)\n",
      "Requirement already satisfied: pandas>=1.3 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[complete]) (2.0.1)\n",
      "Requirement already satisfied: jinja2>=2.10.3 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[complete]) (3.1.2)\n",
      "Requirement already satisfied: bokeh>=2.4.2 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[complete]) (3.1.1)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.5 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed==2023.4.1->dask[complete]) (2.4.0)\n",
      "Requirement already satisfied: psutil>=5.7.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed==2023.4.1->dask[complete]) (5.9.4)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed==2023.4.1->dask[complete]) (1.7.0)\n",
      "Requirement already satisfied: msgpack>=1.0.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed==2023.4.1->dask[complete]) (1.0.5)\n",
      "Requirement already satisfied: tornado>=6.0.3 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed==2023.4.1->dask[complete]) (6.2)\n",
      "Requirement already satisfied: urllib3>=1.24.3 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed==2023.4.1->dask[complete]) (2.0.2)\n",
      "Requirement already satisfied: zict>=2.2.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed==2023.4.1->dask[complete]) (3.0.0)\n",
      "Requirement already satisfied: contourpy>=1 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from bokeh>=2.4.2->dask[complete]) (1.0.7)\n",
      "Requirement already satisfied: pillow>=7.1.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from bokeh>=2.4.2->dask[complete]) (9.5.0)\n",
      "Requirement already satisfied: xyzservices>=2021.09.1 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from bokeh>=2.4.2->dask[complete]) (2023.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from jinja2>=2.10.3->dask[complete]) (2.1.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from pandas>=1.3->dask[complete]) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from pandas>=1.3->dask[complete]) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from pandas>=1.3->dask[complete]) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from python-dateutil>=2.8.2->pandas>=1.3->dask[complete]) (1.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install \"dask[complete]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: dask-ml in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (2023.3.24)\n",
      "Requirement already satisfied: scikit-learn>=1.2.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask-ml) (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask-ml) (1.24.3)\n",
      "Requirement already satisfied: multipledispatch>=0.4.9 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask-ml) (0.6.0)\n",
      "Requirement already satisfied: scipy in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask-ml) (1.10.1)\n",
      "Requirement already satisfied: distributed>=2.4.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask-ml) (2023.4.1)\n",
      "Requirement already satisfied: pandas>=0.24.2 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask-ml) (2.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask-ml) (0.57.0)\n",
      "Requirement already satisfied: dask[array,dataframe]>=2.4.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask-ml) (2023.4.1)\n",
      "Requirement already satisfied: packaging in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask-ml) (23.0)\n",
      "Requirement already satisfied: dask-glm>=0.2.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask-ml) (0.2.0)\n",
      "Requirement already satisfied: cloudpickle>=0.2.2 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask-glm>=0.2.0->dask-ml) (2.2.1)\n",
      "Requirement already satisfied: partd>=1.2.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (1.4.0)\n",
      "Requirement already satisfied: click>=8.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (8.1.3)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (2023.5.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (6.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (6.3.0)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (0.12.0)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed>=2.4.0->dask-ml) (1.7.0)\n",
      "Requirement already satisfied: jinja2>=2.10.3 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed>=2.4.0->dask-ml) (3.1.2)\n",
      "Requirement already satisfied: urllib3>=1.24.3 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed>=2.4.0->dask-ml) (2.0.2)\n",
      "Requirement already satisfied: locket>=1.0.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed>=2.4.0->dask-ml) (1.0.0)\n",
      "Requirement already satisfied: zict>=2.2.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed>=2.4.0->dask-ml) (3.0.0)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.5 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed>=2.4.0->dask-ml) (2.4.0)\n",
      "Requirement already satisfied: msgpack>=1.0.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed>=2.4.0->dask-ml) (1.0.5)\n",
      "Requirement already satisfied: tornado>=6.0.3 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed>=2.4.0->dask-ml) (6.2)\n",
      "Requirement already satisfied: psutil>=5.7.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from distributed>=2.4.0->dask-ml) (5.9.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.13.0->dask[array,dataframe]>=2.4.0->dask-ml) (3.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from jinja2>=2.10.3->distributed>=2.4.0->dask-ml) (2.1.2)\n",
      "Requirement already satisfied: six in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from multipledispatch>=0.4.9->dask-ml) (1.16.0)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from numba>=0.51.0->dask-ml) (0.40.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from pandas>=0.24.2->dask-ml) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from pandas>=0.24.2->dask-ml) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from pandas>=0.24.2->dask-ml) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from scikit-learn>=1.2.0->dask-ml) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages (from scikit-learn>=1.2.0->dask-ml) (1.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install dask-ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.24.3'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning for Spotify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265669"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import relevant libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# import the dataset\n",
    "spotify_tracks = pd.read_csv('archive/tracks.csv')\n",
    "\n",
    "# drop duplicates with the same name and artist\n",
    "spotify_tracks = spotify_tracks.drop_duplicates(\n",
    "  subset = ['name', 'artists'],\n",
    "  keep = 'last').reset_index(drop = True)\n",
    "\n",
    "# convert release_time to appropriate time date format\n",
    "spotify_tracks['release_date']= pd.to_datetime(spotify_tracks['release_date'], format='mixed')\n",
    "\n",
    "# remove songs older than 1990\n",
    "spotify_tracks = spotify_tracks[spotify_tracks['release_date'].dt.year >= 1990]\n",
    "\n",
    "# change duration from ms to minutes\n",
    "spotify_tracks['duration_ms'] = spotify_tracks['duration_ms']/60000\n",
    "\n",
    "# rearrange columns\n",
    "spotify_tracks = spotify_tracks[['id',\n",
    "        'name',\n",
    "        'artists',\n",
    " 'id_artists',\n",
    " 'release_date',\n",
    " 'duration_ms',\n",
    " 'explicit',\n",
    " 'danceability',\n",
    " 'energy',\n",
    " 'key',\n",
    " 'loudness',\n",
    " 'mode',\n",
    " 'speechiness',\n",
    " 'acousticness',\n",
    " 'instrumentalness',\n",
    " 'liveness',\n",
    " 'valence',\n",
    " 'tempo',\n",
    " 'time_signature',\n",
    " 'popularity',\n",
    "]]\n",
    "\n",
    "# reset index\n",
    "spotify_tracks = spotify_tracks.reset_index(drop=True)\n",
    "\n",
    "# identify IQR for duration and remove outliers\n",
    "Q1 = np.percentile(spotify_tracks['duration_ms'], 25,\n",
    "                   method = 'midpoint')\n",
    "Q3 = np.percentile(spotify_tracks['duration_ms'], 75,\n",
    "                   method = 'midpoint')\n",
    "IQR = Q3 - Q1\n",
    "upper = Q3 + 1.5*IQR\n",
    "lower = Q1 - 1.5*IQR\n",
    "upper_array=np.where(spotify_tracks['duration_ms']>=upper)\n",
    "lower_array=np.where(spotify_tracks['duration_ms']<=lower)\n",
    "\n",
    "spotify_tracks.drop(upper_array[0],inplace=True)\n",
    "spotify_tracks.drop(lower_array[0],inplace=True)\n",
    "\n",
    "# remove songs with time signature = 0, 1\n",
    "spotify_tracks = spotify_tracks[(spotify_tracks['time_signature'] != 0) & \n",
    "                                (spotify_tracks['time_signature'] !=1)]\n",
    "\n",
    "# remove songs with high speechiness like talk shows, audio books, poetry\n",
    "spotify_tracks = spotify_tracks[spotify_tracks['speechiness']<0.8]\n",
    "\n",
    "# remove songs with live audiences\n",
    "spotify_tracks = spotify_tracks[spotify_tracks['liveness']<0.9]\n",
    "\n",
    "# drop the artist_id, since we have the artist name\n",
    "spotify_tracks.drop(columns = ['id', 'id_artists'], inplace=True)\n",
    "\n",
    "# drop all null values\n",
    "spotify_tracks = spotify_tracks.dropna()\n",
    "\n",
    "# separate releasedate to month and year and drop releasedate\n",
    "spotify_tracks['month'] = pd.DatetimeIndex(spotify_tracks['release_date']).month\n",
    "spotify_tracks['year'] = pd.DatetimeIndex(spotify_tracks['release_date']).year\n",
    "spotify_tracks.drop(columns = ['release_date'], axis = 1, inplace=True)\n",
    "\n",
    "# it seems like energy/loudness, as well as loudness/acousticness are correlated, and energy/acousticness; decide to remove acousticness and loudness\n",
    "spotify_tracks.drop(columns = ['loudness', 'acousticness'], inplace=True)\n",
    "\n",
    "# ensure that song name and artist name is a string\n",
    "spotify_tracks['name'] = spotify_tracks['name'].astype(str)\n",
    "spotify_tracks['artists'] = spotify_tracks['artists'].astype(str)\n",
    "\n",
    "# remove all non alphanumeric characters in song name and artists\n",
    "spotify_tracks['name'] = spotify_tracks['name'].replace(r'[^A-Za-z0-9\\s]+', '', regex=True)\n",
    "spotify_tracks['artists'] = spotify_tracks['artists'].replace(r'[^A-Za-z0-9\\s]+', '', regex=True)\n",
    "\n",
    "# remove extra spaces in song name and artists\n",
    "spotify_tracks['name'] = spotify_tracks['name'].replace(r'\\s\\s+', ' ', regex=True)\n",
    "spotify_tracks['artists'] = spotify_tracks['artists'].replace(r'\\s\\s+', ' ', regex=True)\n",
    "\n",
    "# remove all special characters, including punctuation\n",
    "spotify_tracks['name'] = spotify_tracks['name'].replace(r'[^\\w\\s]|_', '', regex=True)\n",
    "spotify_tracks['artists'] = spotify_tracks['artists'].replace(r'[^\\w\\s]|_', '', regex=True)\n",
    "\n",
    "# make all characters in song name and artist lowercase\n",
    "spotify_tracks['name'] = spotify_tracks.name.apply(lambda x: x.lower())\n",
    "spotify_tracks['artists'] = spotify_tracks.artists.apply(lambda x: x.lower())\n",
    "\n",
    "# length of spotify_tracks\n",
    "len(spotify_tracks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning for Spotify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "330087"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the dataset\n",
    "billboard_tracks = pd.read_csv('archive/charts.csv')\n",
    "\n",
    "# remove all fields other than song, rank, and artist\n",
    "billboard_tracks.drop(columns = ['date', 'last-week', 'peak-rank', 'weeks-on-board'], inplace=True)\n",
    "\n",
    "# ensure that song name and artist is a string\n",
    "billboard_tracks['song'] = billboard_tracks['song'].astype(str)\n",
    "billboard_tracks['artist'] = billboard_tracks['artist'].astype(str)\n",
    "\n",
    "# remove all non alphanumeric characters in song name and artist\n",
    "billboard_tracks['song'] = billboard_tracks['song'].replace(r'[^A-Za-z0-9\\s]+', '', regex=True)\n",
    "billboard_tracks['artist'] = billboard_tracks['artist'].replace(r'[^A-Za-z0-9\\s]+', '', regex=True)\n",
    "\n",
    "# remove extra spaces in song name and artist\n",
    "billboard_tracks['song'] = billboard_tracks['song'].replace(r'\\s\\s+', ' ', regex=True)\n",
    "billboard_tracks['artist'] = billboard_tracks['artist'].replace(r'\\s\\s+', ' ', regex=True)\n",
    "\n",
    "# remove all special characters, including punctuation\n",
    "billboard_tracks['song'] = billboard_tracks['song'].replace(r'[^\\w\\s]|_', '', regex=True)\n",
    "billboard_tracks['artist'] = billboard_tracks['artist'].replace(r'[^\\w\\s]|_', '', regex=True)\n",
    "\n",
    "# make all characters in song name lowercase\n",
    "billboard_tracks['song'] = billboard_tracks.song.apply(lambda x: x.lower())\n",
    "billboard_tracks['artist'] = billboard_tracks.artist.apply(lambda x: x.lower())\n",
    "\n",
    "# length of billboard_tracks\n",
    "len(billboard_tracks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining the two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure that columns we join on are the same\n",
    "spotify_tracks.rename(columns={'artists': 'artist', 'name': 'song'}, inplace=True)\n",
    "\n",
    "# perform left join\n",
    "combined_tracks = spotify_tracks.merge(billboard_tracks, how = 'left', on = ['song', 'artist'])\n",
    "\n",
    "# replace nan values with zero, if there is no matches from the merge\n",
    "combined_tracks['rank'] = combined_tracks['rank'].replace(np.nan, 0)\n",
    "\n",
    "# convert the rank into binary variable (1 if popular, 0 otherwise)\n",
    "combined_tracks['billboard_popularity'] = np.where(combined_tracks['rank'] > 0, 1, 0)\n",
    "\n",
    "# drop the billboard rank, since we don't want it infuencing our prediction\n",
    "combined_tracks.drop(columns = ['rank'], inplace=True)\n",
    "\n",
    "# drop the artist column, since it was only used for joining\n",
    "combined_tracks.drop(columns=['artist'], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create separate dataset with song names vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of combined_vectsongs: \n",
      "(364000, 16)\n",
      "shape of count_vect_df: \n",
      "(364000, 753)\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# perform count vectorizer (goal is to see if song name has impact on popularity)\n",
    "\n",
    "count_vect = TfidfVectorizer(binary=False, min_df=150)\n",
    "#print(\"init TF idf vectorizer\")\n",
    "name_vectorized = count_vect.fit_transform(combined_tracks['song'])\n",
    "\n",
    "#print(\"created vectorized song names \")\n",
    "\n",
    "# drop the song name column and add the new vectorized song name\n",
    "combined_vectsongs = combined_tracks.drop('song', axis = 1)\n",
    "\n",
    "#print(\" dropped song name column\")\n",
    "\n",
    "count_vect_df = pd.DataFrame(name_vectorized.todense(), columns = count_vect.get_feature_names_out())\n",
    "\n",
    "#print(\"created dataframe from vectorized song names\")\n",
    "\n",
    "# reset index for combined_vectsongs, count_vect_df\n",
    "#print(\"vectsongs shape: \")\n",
    "#print(combined_vectsongs.shape)\n",
    "#print(\" count vect df shape: \")\n",
    "#print(count_vect_df.shape)\n",
    "\n",
    "#combined_vectsongs = combined_vectsongs.reset_index().drop('index', axis = 1)\n",
    "# alternative way of dropping index column\n",
    "combined_vectsongs.reset_index(drop=True, inplace=True)\n",
    "print(\"shape of combined_vectsongs: \")\n",
    "print(combined_vectsongs.shape)\n",
    "\n",
    "#count_vect_df = count_vect_df.reset_index().drop('index', axis = 1)\n",
    "count_vect_df.reset_index(drop=True, inplace=True)\n",
    "print(\"shape of count_vect_df: \")\n",
    "print(count_vect_df.shape)\n",
    "\n",
    "#print(\"combined_vectsongs and count_vect_df reset index\")\n",
    "\n",
    "#combined_vectsongs = pd.concat([combined_vectsongs.reset_index().drop('index', axis = 1), count_vect_df.reset_index().drop('index', axis = 1)], axis = 1)\n",
    "\n",
    "#combined_vectsongs = pd.concat([combined_vectsongs, count_vect_df], axis = 1, ignore_index=True)\n",
    "\n",
    "#combined_vectsongs = combined_vectsongs.join(count_vect_df, lsuffix=\"_left\", rsuffix=\"_right\")\n",
    "\n",
    "\n",
    "#combined_vectsongs = dd.from_pandas(combined_vectsongs, npartitions=8)\n",
    "#count_vect_df = dd.from_pandas(count_vect_df, npartitions=8)\n",
    "\n",
    "combined_vectsongs = pd.concat([combined_vectsongs, count_vect_df], axis = 1)\n",
    "\n",
    "#print(\"concatenated the two dataframes\")\n",
    "# drop the song name from combined dataset as well\n",
    "combined_tracks.drop(columns = ['song'], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating two datasets, two where Spotify popularity is used as target, and the other two where Billboard popularity is used as target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of songs in dataset:  364000\n"
     ]
    }
   ],
   "source": [
    "# create dataset where Spotify popularity is target\n",
    "combined_spotify = combined_tracks.drop(columns = ['billboard_popularity'])\n",
    "combined_spotify_vectsongs = combined_vectsongs.drop(columns = ['billboard_popularity'])\n",
    "\n",
    "print(\"Number of songs in dataset: \", len(combined_spotify))\n",
    "\n",
    "# create dataset where Billboard popularity is target\n",
    "combined_billboard = combined_tracks.drop(columns = ['popularity'])\n",
    "combined_billboard_vectsongs = combined_vectsongs.drop(columns = ['popularity'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model 1 - Linear Regression with Spotify Popularity without song name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score for test set is 0.13918034208167884\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# standardize split the data into training and test sets\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df_train, df_test = train_test_split(combined_spotify, test_size = 0.2)\n",
    "\n",
    "# create the features and target dataframes\n",
    "df_train_x = df_train.drop('popularity', axis = 1).to_numpy()\n",
    "df_train_y = df_train['popularity'].values\n",
    "df_train_x = scaler.fit_transform(df_train_x)\n",
    "\n",
    "df_test_x = df_test.drop('popularity', axis = 1).to_numpy()\n",
    "df_test_y = df_test['popularity'].values\n",
    "df_test_x = scaler.fit_transform(df_test_x)\n",
    "\n",
    "# fit the linear regression model\n",
    "LinReg = LinearRegression()\n",
    "LinReg.fit(df_train_x, df_train_y)\n",
    "\n",
    "# get score on test-set\n",
    "test_score = LinReg.score(df_test_x, df_test_y)\n",
    "\n",
    "# print the score\n",
    "print(f\"R2 score for test set is {test_score}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model 2 - Linear Regression with Spotify Popularity with vectorized song name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train test split done\n",
      "df_train_x created\n",
      "df_train_y created:\n",
      "[22 31 33 ... 24 41 47]\n",
      "df_train_x scaled\n",
      "df_test_x created\n",
      "df_test_y created\n",
      "Linear regression model created\n",
      "type of df_train_y:  <class 'numpy.ndarray'>\n",
      "df_train_x shape:  (291200, 767)\n",
      "df_train_y shape:  (291200,)\n",
      "Linear regression model fitted\n",
      "R2 score for test set is 0.2754341521766862\n"
     ]
    }
   ],
   "source": [
    "# split the data into training and test sets\n",
    "import dask_ml.model_selection as dcv\n",
    "import dask_ml.linear_model as dlm\n",
    "import dask_ml.preprocessing as dpp\n",
    "\n",
    "#print(\"memory usage of combined_spotify_vectsongs:\")\n",
    "#print(combined_spotify_vectsongs.memory_usage(deep=True).sum())\n",
    "#combined_spotify_vectsongs = combined_spotify_vectsongs.compute() \n",
    "#print(\"shape of combined_spotify_vectsongs: \")\n",
    "#print(combined_spotify_vectsongs.shape)\n",
    "#print(\"type of combined_spotify_vectsongs: \")\n",
    "#print(type(combined_spotify_vectsongs))\n",
    "scaler = StandardScaler()\n",
    "df_train, df_test = train_test_split(combined_spotify_vectsongs, test_size = 0.2)\n",
    "print(\"train test split done\")\n",
    "\n",
    "# create the features and target dataframes\n",
    "#df_train_x = df_train.drop('popularity', axis = 1).to_numpy()\n",
    "df_train_x = df_train.drop('popularity', axis = 1).values\n",
    "print(\"df_train_x created\")\n",
    "df_train_y = df_train['popularity'].values\n",
    "print(\"df_train_y created:\")\n",
    "print(df_train_y)\n",
    "df_train_x = scaler.fit_transform(df_train_x)\n",
    "print(\"df_train_x scaled\")\n",
    "\n",
    "#df_test_x = df_test.drop('popularity', axis = 1).to_numpy()\n",
    "df_test_x = df_test.drop('popularity', axis = 1).values\n",
    "print(\"df_test_x created\")\n",
    "df_test_y = df_test['popularity'].values\n",
    "print(\"df_test_y created\")\n",
    "df_test_x = scaler.fit_transform(df_test_x)\n",
    "\n",
    "# fit the linear regression model\n",
    "LinReg = LinearRegression(n_jobs=8)\n",
    "print(\"Linear regression model created\")\n",
    "#print(\"type of df_train_x: \", type(df_train_x))\n",
    "print(\"type of df_train_y: \", type(df_train_y))\n",
    "print(\"df_train_x shape: \", df_train_x.shape)\n",
    "print(\"df_train_y shape: \", df_train_y.shape)\n",
    "LinReg.fit(df_train_x, df_train_y)\n",
    "print(\"Linear regression model fitted\")\n",
    "\n",
    "# get score on test-set\n",
    "test_score = LinReg.score(df_test_x, df_test_y)\n",
    "\n",
    "# print the score\n",
    "print(f\"R2 score for test set is {test_score}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model 3 - Logistic Regression with Billboard Popularity without song name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set with no regularization terms F1-Score is 0.7117032967032967\n",
      "Test set with no regularization terms F1-Score is 0.714478021978022\n"
     ]
    }
   ],
   "source": [
    "# split the data into training and test sets\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df_train, df_test = train_test_split(combined_billboard, test_size = 0.2)\n",
    "\n",
    "# create the features and target dataframes\n",
    "df_train_x = df_train.drop('billboard_popularity', axis = 1).to_numpy()\n",
    "df_train_y = df_train['billboard_popularity'].values\n",
    "df_train_x = scaler.fit_transform(df_train_x)\n",
    "\n",
    "df_test_x = df_test.drop('billboard_popularity', axis = 1).to_numpy()\n",
    "df_test_y = df_test['billboard_popularity'].values\n",
    "df_test_x = scaler.fit_transform(df_test_x)\n",
    "\n",
    "# fit the logistic regression model with no regularization terms\n",
    "LogReg = LogisticRegression(multi_class='ovr', penalty='none', max_iter = 10000)\n",
    "LogReg.fit(df_train_x, df_train_y)\n",
    "\n",
    "# calculate F1 score\n",
    "f1_train = f1_score(df_train_y, LogReg.predict(df_train_x), average = 'micro')\n",
    "f1_test = f1_score(df_test_y, LogReg.predict(df_test_x), average = 'micro')\n",
    "\n",
    "# print F1 values out\n",
    "print(f\"Training set with no regularization terms F1-Score is {f1_train}\")\n",
    "print(f\"Test set with no regularization terms F1-Score is {f1_test}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model 4 - Logistic Regression with Billboard Popularity with vectorized song name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set with no regularization terms F1-Score is 0.8296050824175825\n",
      "Test set with no regularization terms F1-Score is 0.8265934065934066\n"
     ]
    }
   ],
   "source": [
    "# split the data into training and test sets\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df_train, df_test = train_test_split(combined_billboard_vectsongs, test_size = 0.2)\n",
    "\n",
    "# create the features and target dataframes\n",
    "df_train_x = df_train.drop('billboard_popularity', axis = 1).to_numpy()\n",
    "df_train_y = df_train['billboard_popularity'].values\n",
    "df_train_x = scaler.fit_transform(df_train_x)\n",
    "\n",
    "df_test_x = df_test.drop('billboard_popularity', axis = 1).to_numpy()\n",
    "df_test_y = df_test['billboard_popularity'].values\n",
    "df_test_x = scaler.fit_transform(df_test_x)\n",
    "\n",
    "# fit the logistic regression model with no regularization terms\n",
    "LogReg = LogisticRegression(multi_class='ovr', penalty='none', max_iter = 10000)\n",
    "LogReg.fit(df_train_x, df_train_y)\n",
    "\n",
    "# calculate F1 score\n",
    "f1_train = f1_score(df_train_y, LogReg.predict(df_train_x), average = 'micro')\n",
    "f1_test = f1_score(df_test_y, LogReg.predict(df_test_x), average = 'micro')\n",
    "\n",
    "# print F1 values out\n",
    "print(f\"Training set with no regularization terms F1-Score is {f1_train}\")\n",
    "print(f\"Test set with no regularization terms F1-Score is {f1_test}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
