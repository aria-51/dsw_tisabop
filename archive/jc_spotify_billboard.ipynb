{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.24.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning for Spotify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# import the dataset\n",
    "spotify_tracks = pd.read_csv('archive/tracks.csv')\n",
    "\n",
    "# drop duplicates with the same name and artist\n",
    "spotify_tracks = spotify_tracks.drop_duplicates(\n",
    "  subset = ['name', 'artists'],\n",
    "  keep = 'last').reset_index(drop = True)\n",
    "\n",
    "# convert release_time to appropriate time date format\n",
    "spotify_tracks['release_date']= pd.to_datetime(spotify_tracks['release_date'], format='mixed')\n",
    "\n",
    "# remove songs older than 1990\n",
    "spotify_tracks = spotify_tracks[spotify_tracks['release_date'].dt.year >= 1990]\n",
    "\n",
    "# change duration from ms to minutes\n",
    "spotify_tracks['duration_ms'] = spotify_tracks['duration_ms']/60000\n",
    "\n",
    "# rearrange columns\n",
    "spotify_tracks = spotify_tracks[['id',\n",
    "        'name',\n",
    "        'artists',\n",
    " 'id_artists',\n",
    " 'release_date',\n",
    " 'duration_ms',\n",
    " 'explicit',\n",
    " 'danceability',\n",
    " 'energy',\n",
    " 'key',\n",
    " 'loudness',\n",
    " 'mode',\n",
    " 'speechiness',\n",
    " 'acousticness',\n",
    " 'instrumentalness',\n",
    " 'liveness',\n",
    " 'valence',\n",
    " 'tempo',\n",
    " 'time_signature',\n",
    " 'popularity',\n",
    "]]\n",
    "\n",
    "# reset index\n",
    "spotify_tracks = spotify_tracks.reset_index(drop=True)\n",
    "\n",
    "# identify IQR for duration and remove outliers\n",
    "Q1 = np.percentile(spotify_tracks['duration_ms'], 25,\n",
    "                   method = 'midpoint')\n",
    "Q3 = np.percentile(spotify_tracks['duration_ms'], 75,\n",
    "                   method = 'midpoint')\n",
    "IQR = Q3 - Q1\n",
    "upper = Q3 + 1.5*IQR\n",
    "lower = Q1 - 1.5*IQR\n",
    "upper_array=np.where(spotify_tracks['duration_ms']>=upper)\n",
    "lower_array=np.where(spotify_tracks['duration_ms']<=lower)\n",
    "\n",
    "spotify_tracks.drop(upper_array[0],inplace=True)\n",
    "spotify_tracks.drop(lower_array[0],inplace=True)\n",
    "\n",
    "# remove songs with time signature = 0, 1\n",
    "spotify_tracks = spotify_tracks[(spotify_tracks['time_signature'] != 0) & \n",
    "                                (spotify_tracks['time_signature'] !=1)]\n",
    "\n",
    "# remove songs with high speechiness like talk shows, audio books, poetry\n",
    "spotify_tracks = spotify_tracks[spotify_tracks['speechiness']<0.8]\n",
    "\n",
    "# remove songs with live audiences\n",
    "spotify_tracks = spotify_tracks[spotify_tracks['liveness']<0.9]\n",
    "\n",
    "# drop the artist_id, since we have the artist name\n",
    "spotify_tracks.drop(columns = ['id', 'id_artists'], inplace=True)\n",
    "\n",
    "# drop all null values\n",
    "spotify_tracks = spotify_tracks.dropna()\n",
    "\n",
    "# separate releasedate to month and year and drop releasedate\n",
    "spotify_tracks['month'] = pd.DatetimeIndex(spotify_tracks['release_date']).month\n",
    "spotify_tracks['year'] = pd.DatetimeIndex(spotify_tracks['release_date']).year\n",
    "spotify_tracks.drop(columns = ['release_date'], axis = 1, inplace=True)\n",
    "\n",
    "# it seems like energy/loudness, as well as loudness/acousticness are correlated, and energy/acousticness; decide to remove acousticness and loudness\n",
    "spotify_tracks.drop(columns = ['loudness', 'acousticness'], inplace=True)\n",
    "\n",
    "# ensure that song name and artist name is a string\n",
    "spotify_tracks['name'] = spotify_tracks['name'].astype(str)\n",
    "spotify_tracks['artists'] = spotify_tracks['artists'].astype(str)\n",
    "\n",
    "# remove all non alphanumeric characters in song name and artists\n",
    "spotify_tracks['name'] = spotify_tracks['name'].replace(r'[^A-Za-z0-9\\s]+', '', regex=True)\n",
    "spotify_tracks['artists'] = spotify_tracks['artists'].replace(r'[^A-Za-z0-9\\s]+', '', regex=True)\n",
    "\n",
    "# remove extra spaces in song name and artists\n",
    "spotify_tracks['name'] = spotify_tracks['name'].replace(r'\\s\\s+', ' ', regex=True)\n",
    "spotify_tracks['artists'] = spotify_tracks['artists'].replace(r'\\s\\s+', ' ', regex=True)\n",
    "\n",
    "# remove all special characters, including punctuation\n",
    "spotify_tracks['name'] = spotify_tracks['name'].replace(r'[^\\w\\s]|_', '', regex=True)\n",
    "spotify_tracks['artists'] = spotify_tracks['artists'].replace(r'[^\\w\\s]|_', '', regex=True)\n",
    "\n",
    "# make all characters in song name and artist lowercase\n",
    "spotify_tracks['name'] = spotify_tracks.name.apply(lambda x: x.lower())\n",
    "spotify_tracks['artists'] = spotify_tracks.artists.apply(lambda x: x.lower())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning for Spotify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dataset\n",
    "billboard_tracks = pd.read_csv('archive/charts.csv')\n",
    "\n",
    "# remove all fields other than song, rank, and artist\n",
    "billboard_tracks.drop(columns = ['date', 'last-week', 'peak-rank', 'weeks-on-board'], inplace=True)\n",
    "\n",
    "# ensure that song name and artist is a string\n",
    "billboard_tracks['song'] = billboard_tracks['song'].astype(str)\n",
    "billboard_tracks['artist'] = billboard_tracks['artist'].astype(str)\n",
    "\n",
    "# remove all non alphanumeric characters in song name and artist\n",
    "billboard_tracks['song'] = billboard_tracks['song'].replace(r'[^A-Za-z0-9\\s]+', '', regex=True)\n",
    "billboard_tracks['artist'] = billboard_tracks['artist'].replace(r'[^A-Za-z0-9\\s]+', '', regex=True)\n",
    "\n",
    "# remove extra spaces in song name and artist\n",
    "billboard_tracks['song'] = billboard_tracks['song'].replace(r'\\s\\s+', ' ', regex=True)\n",
    "billboard_tracks['artist'] = billboard_tracks['artist'].replace(r'\\s\\s+', ' ', regex=True)\n",
    "\n",
    "# remove all special characters, including punctuation\n",
    "billboard_tracks['song'] = billboard_tracks['song'].replace(r'[^\\w\\s]|_', '', regex=True)\n",
    "billboard_tracks['artist'] = billboard_tracks['artist'].replace(r'[^\\w\\s]|_', '', regex=True)\n",
    "\n",
    "# make all characters in song name lowercase\n",
    "billboard_tracks['song'] = billboard_tracks.song.apply(lambda x: x.lower())\n",
    "billboard_tracks['artist'] = billboard_tracks.artist.apply(lambda x: x.lower())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining the two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure that columns we join on are the same\n",
    "spotify_tracks.rename(columns={'artists': 'artist', 'name': 'song'}, inplace=True)\n",
    "\n",
    "# perform left join\n",
    "combined_tracks = spotify_tracks.merge(billboard_tracks, how = 'left', on = ['song', 'artist'])\n",
    "\n",
    "# replace nan values with zero, if there is no matches from the merge\n",
    "combined_tracks['rank'] = combined_tracks['rank'].replace(np.nan, 0)\n",
    "\n",
    "# convert the rank into binary variable (1 if popular, 0 otherwise)\n",
    "combined_tracks['billboard_popularity'] = np.where(combined_tracks['rank'] > 0, 1, 0)\n",
    "\n",
    "# drop the billboard rank, since we don't want it infuencing our prediction\n",
    "combined_tracks.drop(columns = ['rank'], inplace=True)\n",
    "\n",
    "# drop the artist column, since it was only used for joining\n",
    "combined_tracks.drop(columns=['artist'], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create separate dataset with song names vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform count vectorizer (goal is to see if song name has impact on popularity)\n",
    "count_vect = CountVectorizer(binary=False, min_df=100)\n",
    "name_vectorized = count_vect.fit_transform(combined_tracks['song'])\n",
    "\n",
    "# drop the song name column and add the new vectorized song name\n",
    "combined_vectsongs = combined_tracks.drop('song', axis = 1)\n",
    "count_vect_df = pd.DataFrame(name_vectorized.todense(), columns = count_vect.get_feature_names_out())\n",
    "combined_vectsongs = pd.concat([combined_vectsongs.reset_index().drop('index', axis = 1), count_vect_df.reset_index().drop('index', axis = 1)], axis = 1)\n",
    "\n",
    "# drop the song name from combined dataset as well\n",
    "combined_tracks.drop(columns = ['song'], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating two datasets, two where Spotify popularity is used as target, and the other two where Billboard popularity is used as target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset where Spotify popularity is target\n",
    "combined_spotify = combined_tracks.drop(columns = ['billboard_popularity'])\n",
    "combined_spotify_vectsongs = combined_vectsongs.drop(columns = ['billboard_popularity'])\n",
    "\n",
    "# create dataset where Billboard popularity is target\n",
    "combined_billboard = combined_tracks.drop(columns = ['popularity'])\n",
    "combined_billboard_vectsongs = combined_vectsongs.drop(columns = ['popularity'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model 1 - Linear Regression with Spotify Popularity without song name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score for test set is 0.13905156863722867\n"
     ]
    }
   ],
   "source": [
    "# split the data into training and test sets\n",
    "df_train, df_test = train_test_split(combined_spotify, test_size = 0.2)\n",
    "\n",
    "# create the features and target dataframes\n",
    "df_train_x = df_train.drop('popularity', axis = 1).to_numpy()\n",
    "df_train_y = df_train['popularity'].values\n",
    "\n",
    "df_test_x = df_test.drop('popularity', axis = 1).to_numpy()\n",
    "df_test_y = df_test['popularity'].values\n",
    "\n",
    "# fit the linear regression model\n",
    "LinReg = LinearRegression()\n",
    "LinReg.fit(df_train_x, df_train_y)\n",
    "\n",
    "# get score on test-set\n",
    "test_score = LinReg.score(df_test_x, df_test_y)\n",
    "\n",
    "# print the score\n",
    "print(f\"R2 score for test set is {test_score}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model 2 - Linear Regression with Spotify Popularity with vectorized song name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score for test set is 0.29140443537517435\n"
     ]
    }
   ],
   "source": [
    "# split the data into training and test sets\n",
    "df_train, df_test = train_test_split(combined_spotify_vectsongs, test_size = 0.2)\n",
    "\n",
    "# create the features and target dataframes\n",
    "df_train_x = df_train.drop('popularity', axis = 1).to_numpy()\n",
    "df_train_y = df_train['popularity'].values\n",
    "\n",
    "df_test_x = df_test.drop('popularity', axis = 1).to_numpy()\n",
    "df_test_y = df_test['popularity'].values\n",
    "\n",
    "# fit the linear regression model\n",
    "LinReg = LinearRegression()\n",
    "LinReg.fit(df_train_x, df_train_y)\n",
    "\n",
    "# get score on test-set\n",
    "test_score = LinReg.score(df_test_x, df_test_y)\n",
    "\n",
    "# print the score\n",
    "print(f\"R2 score for test set is {test_score}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model 3 - Logistic Regression with Billboard Popularity without song name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set with no regularization terms F1-Score is 0.7097802197802198\n",
      "Test set with no regularization terms F1-Score is 0.708708791208791\n"
     ]
    }
   ],
   "source": [
    "# split the data into training and test sets\n",
    "df_train, df_test = train_test_split(combined_billboard, test_size = 0.2)\n",
    "\n",
    "# create the features and target dataframes\n",
    "df_train_x = df_train.drop('billboard_popularity', axis = 1).to_numpy()\n",
    "df_train_y = df_train['billboard_popularity'].values\n",
    "\n",
    "df_test_x = df_test.drop('billboard_popularity', axis = 1).to_numpy()\n",
    "df_test_y = df_test['billboard_popularity'].values\n",
    "\n",
    "# fit the logistic regression model with no regularization terms\n",
    "LogReg = LogisticRegression(multi_class='ovr', penalty='none', max_iter = 10000)\n",
    "LogReg.fit(df_train_x, df_train_y)\n",
    "\n",
    "# calculate F1 score\n",
    "f1_train = f1_score(df_train_y, LogReg.predict(df_train_x), average = 'micro')\n",
    "f1_test = f1_score(df_test_y, LogReg.predict(df_test_x), average = 'micro')\n",
    "\n",
    "# print F1 values out\n",
    "print(f\"Training set with no regularization terms F1-Score is {f1_train}\")\n",
    "print(f\"Test set with no regularization terms F1-Score is {f1_test}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model 4 - Logistic Regression with Billboard Popularity with vectorized song name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhinavgirish/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set with no regularization terms F1-Score is 0.8127026098901099\n",
      "Test set with no regularization terms F1-Score is 0.8119093406593406\n"
     ]
    }
   ],
   "source": [
    "# split the data into training and test sets\n",
    "df_train, df_test = train_test_split(combined_billboard_vectsongs, test_size = 0.2)\n",
    "\n",
    "# create the features and target dataframes\n",
    "df_train_x = df_train.drop('billboard_popularity', axis = 1).to_numpy()\n",
    "df_train_y = df_train['billboard_popularity'].values\n",
    "\n",
    "df_test_x = df_test.drop('billboard_popularity', axis = 1).to_numpy()\n",
    "df_test_y = df_test['billboard_popularity'].values\n",
    "\n",
    "# fit the logistic regression model with no regularization terms\n",
    "LogReg = LogisticRegression(multi_class='ovr', penalty='none', max_iter = 10000)\n",
    "LogReg.fit(df_train_x, df_train_y)\n",
    "\n",
    "# calculate F1 score\n",
    "f1_train = f1_score(df_train_y, LogReg.predict(df_train_x), average = 'micro')\n",
    "f1_test = f1_score(df_test_y, LogReg.predict(df_test_x), average = 'micro')\n",
    "\n",
    "# print F1 values out\n",
    "print(f\"Training set with no regularization terms F1-Score is {f1_train}\")\n",
    "print(f\"Test set with no regularization terms F1-Score is {f1_test}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
